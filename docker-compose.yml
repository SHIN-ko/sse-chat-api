version: "3.9"

services:
  llama_cpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama_cpp
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models:ro
    command:
      - -m
      - /models/Dolphin3.0-Llama3.1-8B-Q8_0.gguf      # ← 여기에 넣은 gguf 파일명으로 바꾸세요
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --alias
      - local-llama